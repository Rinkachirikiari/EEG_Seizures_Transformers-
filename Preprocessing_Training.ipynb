{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "import mne\n",
    "import matplotlib as plt \n",
    "import os \n",
    "import mne\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "from torchvision.models import vgg19\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation & Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Chemin vers le dossier parent\n",
    "dossier_parent = \"C:/Users/MAREZ10/OneDrive - Université Laval/Bureau/Projet Transformers/eeg_data\"\n",
    "\n",
    "# Dictionnaire pour stocker les données MNE par sous-dossier\n",
    "donnees_par_sous_dossier = {}\n",
    "\n",
    "# Parcours des sous-dossiers\n",
    "for nom_sous_dossier in os.listdir(dossier_parent):\n",
    "    \n",
    "    chemin_sous_dossier = os.path.join(dossier_parent, nom_sous_dossier)\n",
    "    \n",
    "    if os.path.isdir(chemin_sous_dossier):\n",
    "        \n",
    "        donnees_par_sous_dossier[nom_sous_dossier] = []\n",
    "        \n",
    "        for nom_fichier in os.listdir(chemin_sous_dossier):\n",
    "            \n",
    "            if nom_fichier.endswith('.edf'):\n",
    "                \n",
    "                chemin_fichier = os.path.join(chemin_sous_dossier, nom_fichier)\n",
    "                donnees_mne = mne.io.read_raw_edf(chemin_fichier)\n",
    "                donnees_par_sous_dossier[nom_sous_dossier].append(donnees_mne)\n",
    "\n",
    "# Affichage des informations sur les données MNE\n",
    "for nom_sous_dossier, donnees_mne in donnees_par_sous_dossier.items():\n",
    "    print(f\"Sujet {nom_sous_dossier} : {len(donnees_mne)} fichiers .edf\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(donnees_par_sous_dossier.keys())\n",
    "Y = list(len(elem) for elem in donnees_par_sous_dossier.values())\n",
    "\n",
    "# Création de l'histogramme\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(X,Y, color='skyblue')\n",
    "plt.xlabel('Sujets')\n",
    "plt.ylabel('Nombre d\\'enregistrements')\n",
    "plt.title('Nombre d\\'enregistrements par sujet')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotation des étiquettes sur l'axe des x pour une meilleure lisibilité\n",
    "plt.tight_layout()  # Ajustement automatique du tracé pour éviter les chevauchements\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temps d'échantillonnage - Nombre d'enregistrements n'ayant pas une durée de 1 heure par sujet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire pour stocker le nombre de fichiers par sous-dossier\n",
    "nombre_par_sous_dossier = {}\n",
    "\n",
    "# Parcours des sous-dossiers\n",
    "for nom_sous_dossier, donnees_mne_liste in donnees_par_sous_dossier.items():\n",
    "    # Initialisation du compteur pour ce sous-dossier\n",
    "    cpt = 0\n",
    "    # Parcours des données MNE dans ce sous-dossier\n",
    "    for donnees_mne in donnees_mne_liste:\n",
    "        # Vérification si la durée de la donnée MNE est inférieure à la valeur donnée\n",
    "        if donnees_mne.times[-1] < 3599.99609375:\n",
    "            # Incrémentation du compteur\n",
    "            cpt += 1\n",
    "    # Stockage du nombre dans le dictionnaire\n",
    "    nombre_par_sous_dossier[nom_sous_dossier] = cpt\n",
    "\n",
    "# Affichage du nombre de fichiers qui satisfont la condition pour chaque sous-dossier\n",
    "for nom_sous_dossier, nombre in nombre_par_sous_dossier.items():\n",
    "    print(f\"Sous-dossier {nom_sous_dossier} : {nombre} fichiers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chemin d'accès au fichier\n",
    "file_path = \"C:/Users/MAREZ10/OneDrive - Université Laval/Bureau/Projet Transformers/eeg_data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_summary(file_path):\n",
    "    # Initialisation des listes pour stocker les informations\n",
    "    seizure_presence = {}\n",
    "    \n",
    "    # Chemin d'accès au fichier\n",
    "    #file_path = \"chb04-summary.txt\"\n",
    "    \n",
    "    # Ouvrir et lire le fichier\n",
    "    is_seizure = False\n",
    "    all_files_str = []\n",
    "    seizure_start = 0\n",
    "    with open(file_path, \"r\") as file:\n",
    "        # Lire chaque ligne du fichier\n",
    "        for line in file:\n",
    "            # Traiter la ligne actuelle\n",
    "            if line.strip():\n",
    "                if \"File Name:\" in line:\n",
    "                  start = len(\"File Name: \")\n",
    "                  all_files_str.append(str(line[start:len(line)-1]))\n",
    "                elif (\"Seizure\" in line) and (\"Start Time: \" in line):\n",
    "                  if line[len(\"Seizure S\")-1] == \"S\":\n",
    "                    start = len(\"Seizure Start Time: \")\n",
    "                  else:\n",
    "                    start = len(\"Seizure 1 Start Time: \")\n",
    "                  end = len(\" seconds\")+1\n",
    "                  seizure_start = int(line[start:len(line)-end])\n",
    "                elif (\"Seizure\"in line) and (\"End Time: \" in line):\n",
    "                  if line[len(\"Seizure E\")-1] == \"E\":\n",
    "                    start = len(\"Seizure End Time: \")\n",
    "                  else:\n",
    "                    start = len(\"Seizure 1 End Time: \")\n",
    "                  end = len(\" seconds\")+1\n",
    "                  seizure_end = int(line[start:len(line)-end])\n",
    "                  if not all_files_str[len(all_files_str)-1] in seizure_presence.keys():\n",
    "                    seizure_presence[all_files_str[len(all_files_str)-1]] = []\n",
    "                  seizure_presence[all_files_str[len(all_files_str)-1]].append((seizure_start, seizure_end))\n",
    "    print(\"Seizure presence init\" ,seizure_presence)\n",
    "    return seizure_presence, all_files_str\n",
    "\n",
    "\n",
    "def separate_data_intervals(file_str, seizure_presence,path):\n",
    "  # Durée de chaque intervalle en secondes (2 minutes)\n",
    "  interval_duration = 60\n",
    "\n",
    "  raw = mne.io.read_raw_edf(path + file_str)\n",
    "  #data, times = raw[:, :]\n",
    "  total_duration = raw.times[-1] # en secondes\n",
    "\n",
    "  # Nombre total d'intervalle de 10 minutes\n",
    "  num_intervals = int(total_duration / interval_duration)\n",
    "  labels = []\n",
    "  data = []\n",
    "  \n",
    "  # print(\"Seizure presence: \", seizure_presence) \n",
    "  \n",
    "  # Diviser les données en intervalles de 10 minutes\n",
    "  for i in range(num_intervals):\n",
    "      # Calculer le temps de début et de fin de chaque intervalle\n",
    "      start_time = i * interval_duration\n",
    "      end_time = (i + 1) * interval_duration\n",
    "            \n",
    "\n",
    "      # Convertir le temps en indice\n",
    "      start_idx = raw.time_as_index(start_time)\n",
    "      end_idx = raw.time_as_index(end_time)\n",
    "\n",
    "      # Extraire les données de l'intervalle\n",
    "      interval_data, interval_times = raw[:, start_idx:end_idx]\n",
    "      data.append(interval_data)\n",
    "\n",
    "      \n",
    "      if file_str in seizure_presence.keys():\n",
    "        is_seizure = False\n",
    "        for start_seizure, end_seizure in seizure_presence[file_str]:\n",
    "\n",
    "\n",
    "          if (start_seizure >= start_time and start_seizure <= end_time) or (end_seizure >= start_time and end_seizure <= end_time) or (start_seizure <= start_time and end_seizure >= end_time):\n",
    "                \n",
    "            is_seizure = True\n",
    "  \n",
    "            break\n",
    "        if is_seizure:\n",
    "          labels.append(1)\n",
    "        else:\n",
    "          labels.append(0)\n",
    "      else:\n",
    "        labels.append(0)\n",
    "  return data, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_for_patient(file_path, patient):\n",
    "      \n",
    "    path = file_path +\"/\"+ patient + \"/\"\n",
    "    seizure_presence, all_files_str = read_summary(path+patient+\"-summary.txt\")\n",
    "    data = []\n",
    "    labels = []\n",
    "    for file in all_files_str:\n",
    "      print(\"Reading file : \", file)\n",
    "      interval_data, label = separate_data_intervals(file, seizure_presence,path)\n",
    "      print(\"End of file\")\n",
    "      for i in range(len(interval_data)):\n",
    "        data.append(interval_data[i])\n",
    "        labels.append(label[i])\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = load_data_for_patient(file_path, \"chb01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 2377, 1: 15}\n"
     ]
    }
   ],
   "source": [
    "oc={}\n",
    "for elem in labels:\n",
    "    oc[elem] = labels.count(elem)\n",
    "\n",
    "print(oc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data : \n",
    "    print(item.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1913, 23, 15360)\n",
      "(1913,)\n",
      "(479, 23, 15360)\n",
      "(479,)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'convolution' from 'c:\\\\Users\\\\MAREZ10\\\\OneDrive - Université Laval\\\\Documents\\\\GitHub\\\\EEG_Seizures_Transformers-\\\\convolution.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "import convolution\n",
    "from importlib import reload\n",
    "reload(transformers)\n",
    "reload(convolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Conformer(nn.Sequential):\n",
    "    def __init__(self, emb_size=40, nb_channels =23, depth=6, n_classes=2, **kwargs):\n",
    "        super().__init__(\n",
    "\n",
    "            convolution.PatchEmbedding(emb_size, nb_channels),\n",
    "            transformers.TransformerEncoder(depth, emb_size),\n",
    "            transformers.ClassificationHead(emb_size, n_classes)\n",
    "        )\n",
    "\n",
    "batch_size = 4\n",
    "n_epochs = 100\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model = Conformer().cuda()\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.5, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data, train_label, test_data, test_label):\n",
    "\n",
    "        \n",
    "        train_data = torch.from_numpy(train_data)\n",
    "        train_label = torch.from_numpy(train_label)\n",
    "        dataset = torch.utils.data.TensorDataset(train_data, train_label)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        test_data = torch.from_numpy(test_data)\n",
    "        test_label = torch.from_numpy(test_label)\n",
    "        #test_dataset = torch.utils.data.TensorDataset(test_data, test_label)\n",
    "        #test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "        test_data = Variable(test_data.type(Tensor))\n",
    "        test_label = Variable(test_label.type(Tensor))\n",
    "\n",
    "        bestAcc = 0\n",
    "        averAcc = 0\n",
    "        num = 0\n",
    "        Y_true = 0\n",
    "        Y_pred = 0\n",
    "\n",
    "\n",
    "\n",
    "        for e in range(n_epochs):\n",
    "            model.train()\n",
    "            for i, (train_data, train_label) in enumerate(dataloader):\n",
    "\n",
    "                train_data = Variable(train_data.cuda().type(Tensor))\n",
    "                train_label = Variable(train_label.cuda().type(Tensor))\n",
    "\n",
    "                tok, outputs = model(train_data)\n",
    "\n",
    "                loss = criterion(torch.argmax(outputs,dim=1), train_label) \n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # test process\n",
    "            if (e + 1) % 1 == 0:\n",
    "                model.eval()\n",
    "                Tok, Cls = model(test_data)\n",
    "\n",
    "\n",
    "                loss_test = criterion(Cls, test_label)\n",
    "                y_pred = torch.max(Cls, 1)[1]\n",
    "                acc = float((y_pred == test_label).cpu().numpy().astype(int).sum()) / float(test_label.size(0))\n",
    "                train_pred = torch.max(outputs, 1)[1]\n",
    "                train_acc = float((train_pred == train_label).cpu().numpy().astype(int).sum()) / float(train_label.size(0))\n",
    "\n",
    "                print('Epoch:', e,\n",
    "                      '  Train loss: %.6f' % loss.detach().cpu().numpy(),\n",
    "                      '  Test loss: %.6f' % loss_test.detach().cpu().numpy(),\n",
    "                      '  Train accuracy %.6f' % train_acc,\n",
    "                      '  Test accuracy is %.6f' % acc)\n",
    "\n",
    "                \n",
    "                num = num + 1\n",
    "                averAcc = averAcc + acc\n",
    "                if acc > bestAcc:\n",
    "                    bestAcc = acc\n",
    "                    Y_true = test_label\n",
    "                    Y_pred = y_pred\n",
    "\n",
    "\n",
    "        torch.save(model.module.state_dict(), 'model.pth')\n",
    "        averAcc = averAcc / num\n",
    "        print('The average accuracy is:', averAcc)\n",
    "        print('The best accuracy is:', bestAcc)\n",
    "        \n",
    "\n",
    "        return bestAcc, averAcc, Y_true, Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_data, train_label, test_data, test_label)\u001b[0m\n\u001b[0;32m     10\u001b[0m test_label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(test_label)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#test_dataset = torch.utils.data.TensorDataset(test_data, test_label)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m test_data \u001b[38;5;241m=\u001b[39m Variable(\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     16\u001b[0m test_label \u001b[38;5;241m=\u001b[39m Variable(test_label\u001b[38;5;241m.\u001b[39mtype(Tensor))\n\u001b[0;32m     18\u001b[0m bestAcc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "train(train_data, train_labels, test_data, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
